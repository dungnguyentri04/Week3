# üìù Big Data Report: X·ª≠ L√Ω Dataset Titanic v·ªõi Apache Spark & Hadoop
## 1. Hadoop
### Hadoop HDFS

- H·ªá th·ªëng file ph√¢n t√°n.
- L∆∞u tr·ªØ d·ªØ li·ªáu d√πng cho Apache Spark.

### C√°c th√†nh ph·∫ßn hadoop
- NameNode: Qu·∫£n l√Ω metadata (th√¥ng tin v·ªÅ t·∫≠p tin, th∆∞ m·ª•c, block).
- DataNode: L∆∞u tr·ªØ th·ª±c t·∫ø d·ªØ li·ªáu (block).
- SecondaryNameNode: H·ªó tr·ª£ backup metadata ƒë·ªÉ gi·∫£m t·∫£i cho NameNode.
- ResourceManager (YARN): Qu·∫£n l√Ω t√†i nguy√™n v√† l·ªãch tr√¨nh t√°c v·ª•.
- NodeManager: Qu·∫£n l√Ω t√†i nguy√™n c·ª• th·ªÉ tr√™n t·ª´ng m√°y worker.

## 2. Apache Spark

- N·ªÅn t·∫£ng x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn nhanh, h·ªó tr·ª£ in-memory computation.
- H·ªó tr·ª£ RDD v√† DataFrame API.

### Spark vs Hadoop MapReduce
MapReduce (MR) :
- Input ƒë∆∞·ª£c ƒë·ªçc t·ª´ HDFS ‚Üí x·ª≠ l√Ω ‚Üí output ghi l·∫°i v√†o HDFS ‚Üí l·∫∑p l·∫°i c√°c b∆∞·ªõc n√†y cho ƒë·∫øn khi ho√†n th√†nh.
- Input ƒë∆∞·ª£c chia th√†nh c√°c block ƒë·ªôc l·∫≠p ‚Üí c√°c task Map/Reduce ƒë∆∞·ª£c th·ª±c hi·ªán song song ‚Üí ph√π h·ª£p x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn.

H·∫°n ch·∫ø c·ªßa MapReduce:
- M·ªói b∆∞·ªõc x·ª≠ l√Ω ƒë·ªÅu ghi d·ªØ li·ªáu ra HDFS, g√¢y t·ªën th·ªùi gian I/O do ph·ª• thu·ªôc v√†o disk.
- G√¢y ra v·∫•n ƒë·ªÅ v·ªÅ l∆∞u tr·ªØ, replication, v√† ƒë·ªô tr·ªÖ trong x·ª≠ l√Ω.
- Code kh√≥ ph√°t tri·ªÉn v√† debug do t√≠nh ph·ª©c t·∫°p, d√†i d√≤ng.

Spark
- S·ª≠ d·ª•ng kh√°i ni·ªám RDD (Resilient Distributed Dataset):
    - L√† t·∫≠p d·ªØ li·ªáu b·∫•t bi·∫øn, ph√¢n t√°n, l∆∞u ·ªü read-only memory, x·ª≠ l√Ω song song.
- Input ch·ªâ c·∫ßn load 1 l·∫ßn t·ª´ storage, c√°c b∆∞·ªõc x·ª≠ l√Ω ƒë∆∞·ª£c l·∫≠p k·∫ø ho·∫°ch t·ªëi ∆∞u v√† th·ª±c thi li√™n t·ª•c.
- Ch·∫°y ch·ªß y·∫øu tr√™n RAM ‚Üí t·∫≠n d·ª•ng I/O t·ªëc ƒë·ªô cao ‚Üí khi thi·∫øu RAM m·ªõi ghi ra disk.
- Hi·ªáu su·∫•t cao h∆°n MapReduce t·ª´ 10‚Äì100 l·∫ßn, do:
    - Gi·∫£m thao t√°c ghi ƒë·ªçc disk.
    - TƒÉng t·ªëc ƒë·ªô x·ª≠ l√Ω nh·ªù th·ª±c hi·ªán tr√™n b·ªô nh·ªõ.


### ƒê·ªãnh d·∫°ng file

- CSV: Text thu·∫ßn, kh√¥ng h·ªó tr·ª£ schema, kh√¥ng n√©n.
- Parquet: File theo c·ªôt, h·ªó tr·ª£ schema, n√©n t·ªët.
- ORC: C·ªôt, t·ªëi ∆∞u cho Hive, n√©n t·ªët nh·∫•t.

## 4. D·ªØ li·ªáu

- File: `titanic.csv`
- T·∫°o b·ªô d·ªØ li·ªáu 1GB+: ƒë·ªÉ th·ª±c h√†nh v·ªõi Spark.

## 5. C·∫•u h√¨nh v√† ch·∫°y c√°c th√†nh ph·∫ßn hadoop

## 5. Upload d·ªØ li·ªáu l√™n HDFS
```bash
hdfs dfs -mkdir -p /input 
hdfs dfs -put converted-file.csv /input
```

## 6. X·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Spark (Java)
### Kh·ªüi t·∫°o SparkSession
```java
SparkSession spark = SparkSession.builder()
    .appName("SparkFirstProgram")
    .config("spark.master", "local[*]")
    .getOrCreate();
```
- Kh·ªüi t·∫°o m·ªôt phi√™n l√†m vi·ªác v·ªõi Spark.
- `local[*]` cho ph√©p s·ª≠ d·ª•ng t·∫•t c·∫£ c√°c core c√≥ s·∫µn tr√™n m√°y ƒë·ªÉ th·ª±c thi.

### ƒê·ªçc file CSV t·ª´ HDFS
```java
Dataset<Row> dataset = spark.read()
    .option("header", "true")
    .option("inferSchema", "true")
    .option("nullValue", "null")
    .csv("hdfs://localhost:9000/input/converted-file.csv");
```
- ƒê·ªçc t·∫≠p tin CSV tr√™n HDFS.

### L·∫•y d·ªØ li·ªáu ph√¢n t√≠ch t·ªïng qu√°t
```java
Dataset<Row> summary = dataset.describe();
summary.coalesce(1)
    .write()
    .option("header", "true")
    .csv("hdfs://localhost:9000/output/summary_1");
```
- `describe()` sinh ra th·ªëng k√™ t·ªïng qu√°t: count, mean, stddev, min, max cho m·ªói c·ªôt.
- `coalesce(1)` gom t·∫•t c·∫£ d·ªØ li·ªáu v·ªÅ 1 file duy nh·∫•t.

### Ph√¢n t√≠ch theo gi·ªõi t√≠nh & h·∫°ng v√©
```java
Dataset<Row> summaryTable = dataset
    .groupBy("Sex", "Pclass")
    .agg(
        count("").alias("Total"),
        sum("Survived").alias("Survived"),
        expr("avg(Survived)  100").alias("SurvivalRate"),
        avg("Age").alias("AvgAge"),
        avg("Fare").alias("AvgFare")
    )
    .orderBy("Sex", "Pclass");
```
- T√≠nh to√°n t·ªïng s·ªë ng∆∞·ªùi, s·ªë ng∆∞·ªùi s·ªëng s√≥t, t·ª∑ l·ªá s·ªëng s√≥t trung b√¨nh, tu·ªïi v√† gi√° v√© trung b√¨nh.
- S·∫Øp x·∫øp k·∫øt qu·∫£ ƒë·ªÉ d·ªÖ theo d√µi.

### Ghi k·∫øt qu·∫£ ra HDFS
```java
summaryTable.coalesce(1)
    .write()
    .option("header", "true")
    .csv("hdfs://localhost:9000/output/summary_2");
```
- Ghi k·∫øt qu·∫£ ph√¢n t√≠ch xu·ªëng HDFS.
- D·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u d∆∞·ªõi d·∫°ng CSV, c√≥ header, gom th√†nh 1 file duy nh·∫•t.

## 7. Bi√™n d·ªãch v√† ch·∫°y

```bash
mvn clean package 
spark-submit --class com.spark.Main target/spark-demo-1.0.jar
```

## 7. Output

## 8. K·∫øt qu·∫£ & So s√°nh


## 9. K·∫øt lu·∫≠n


